%%
\section{Foundations of Machine Learning}
%
Machine Learning is a research field that emerged from the field of \emph{artificial intelligence}. %The aim of Machine Learning is defined to \textit{give a computer the ability to learn without being explicitly programmed}. %
There are two general definitions of machine learning from Arthur Samuel and Tom Mitchell available: %
%%================================================================================================%%
\leavevmode\\[1ex]
\begin{quote}
   ``Field of study that gives computers the ability to learn without being explicitly programmed.''\newline%
   \raggedleft--- \textup{Arthur Samuel}, 1959
\end{quote}
\leavevmode\\[.25ex]
\begin{quote}
   ``A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.''\newline%
   \raggedleft--- \textup{Tom Mitchell}, 1997
\end{quote}
\leavevmode\\[1ex]
%%================================================================================================%%
The latter definition is more often cited and a more formal and modern definition. %
Here we constrain ourselves to the definition that a machine \emph{learns} to perform a \emph{task} from \emph{experience}. %
More formally, learning means to adjust or to tune parameter values (often called $\theta$ or $w$), a task is considered to be a function $f$, and experience is represented by data (generally we use $\mathcal{D}$ to be our dataset because the actual form of the data set depends on the issue we are dealing with). %
Using the just introduced terminology, we grasp learning problems in the following mathematical manner %
\begin{equation}
   y = f(x;\mathbf{w})
   \label{eq:machine_learning}
\end{equation} where $y$ is called \textit{output variable}, $x$ is called \textit{input variable}, and $\mathbf{w}$ are the \textit{model parameters}. %
Equation \ref{eq:machine_learning} shows the special case of \emph{supervised learning}. %
%Hence, \textbf{learning} a \textbf{task} is based on \textbf{data}; thus we adjust the parameters to obtain output variables that relate to the input variables. %
%%================================================================================================%%
\subsection{Supervised Learning}
In supervised learning, we deal with data of the form $\mathcal{D} = \left\{ (x_1,y_1),(x_2,y_2), \ldots, (x_N,y_N) \right\}$. %
Reconsider Equation \ref{eq:machine_learning}; we try to find a continuous function $f$ that maps any input to an output. %
Hence, the aim of supervised learning is to find a model that is sufficiently precise enough to fit the data $\mathcal{D}$ during training and \emph{generalize well} to new unseen data. %
\\[2ex]%
\textbf{Example:} More common, we are presented some input data $x$ and its related outcome $y$. %
Assume that a teacher shows you multiple pictures of cars such as a red Ferraris, black Porsches, blue BMWs, and so on. %
In this case, our input data (or features) might be the color of the cars and the related outcome is the car name (e.g. Ferrari, Porsche, BMW, etc.). %
In the beginning of your supervised learning task, your teacher shows the pictures of the cars to you and let you know about the outcomes $y$. %
After a while, you should have learned a function $f$ that maps new unseen colored cars to their type. %
%%================================================================================================%%
\subsection{Unsupervised Learning}
In unsupervised learning, we deal with data of the form $\mathcal{D} = \left\{ x_1,x_2, \ldots, x_N \right\}$. %
Given only input data or features $x$; we try to find patterns in the data. %

\begin{itemize}
	\item\itab{Given}\tab{Input data points $x$}
	\item\itab{Goals}%
	\begin{itemize}
		\item\itab{Clustering}
			\htab{find groups in the data}
		\item\itab{Density estimation}
			\htab{determine the data distribution $p(x)$}
		\item\itab{Dimensionality reduction}
			\htab{visualize the data by projections}
	\end{itemize}
\end{itemize}
%%================================================================================================%%
\subsection{Semi-supervised Learning}\label{ssec:SSL}
We are given two datasets %
\begin{equation}
\mathcal{D}_l = \left\{(x_1,y_1),\ldots(x_k,y_k)\right\}
\label{eq:SSL_train_set}
\end{equation}
for learning and %
\begin{equation}
\mathcal{D}_t = \left\{x_k,\ldots,x_{k+n}\right\}
\label{eq:SSL_test_set}
\end{equation}
for testing. %
Again, we seek the solution given by Equation~(\ref{eq:goal_ml}). %
How can we use the extra information of $\mathcal{D}_t$ for this purpose? %
We shall use the test set to evaluate our model. %
%%================================================================================================%%
\subsection{Reinforcement Learning}
\begin{itemize}
	\item\itab{Given}
	\begin{itemize}
		\item\itab{Exploration}%
			\tab{try out new actions}
		\item\itab{Exploitation}%
			\tab{use known actions that yield a high reward}
	\end{itemize}
	\item\itab{Goals}%
	\begin{itemize}
		\item Find a good trade-off between \textbf{Exploration} and \textbf{Exploitation} to maximize the reward for all actions
	\end{itemize}
\end{itemize}
%
%
%
\subsection{Transductive Learning}
Transductive learning is similar to semi-supervised learning of Section~(\ref{ssec:SSL}). %
In contrast to semi-supervised learning, we do not try to learn a function $f$~(according to Equation~(\ref{eq:goal_ml})) explicitly but rather try to get predicitions for the test data set given by Equation~(\ref{eq:SSL_test_set}). %
Moreover, the test data set can be used for training. %
%
%
%
\subsection{On-line Learning}
The training data is presented step-by-step; hence we are given a new datapoint of the form $x_t$ or $(x_t,y_t)$ at each new time step $t$. %
In this scenario, we have limited storage and need to learn in epochs where we only use a limited set of training data points (mostly those datapoints that help to achieve a certain goal). %
%
%
%
\subsection{Large-scale Learning}
There is no definition for the term large-scale but it refers to \textit{huge} amounts of data which is mostly used by fast or parallelized learning algorithms. %
%
%
%
\subsection{Active Learning}
We have a data set similar to the one in Equation~(\ref{eq:unsupervised_train_set}) and our goal is to learn $y$ as in Equation~(\ref{eq:goal_ml}). %
We suppose that labeling a data point $x_i$, i.e. predict $y_i$, costs something. %
The goal is to keep the cost as low as possible. %
%
%
%
\subsection{Structured Output Learning}
For structured output learning, we try to learn a function %
\begin{equation}
f(x,y;\mathbf{w})
\label{eq:goal_structured_output_learning}
\end{equation}%
which we can use to predict a certain outcome %
\begin{equation}
y = \underset{\bar{y}}{\arg\max} \left\{ f(x,\bar{y};\mathbf{w}) \right\}.
\end{equation}
For the purpose to obtain an optimal $y$ we are given a data set as of the form in Equation~(\ref{eq:supervised_train_set}). %
The wording structure in this context relates to e.g. graphical models, trees, etc.. %
%
%
%
\section{Decision Theory -- Inference and decision}
\begin{minipage}{1\textwidth}
	\vspace*{5pt}
	\begin{minipage}[t]{.48\textwidth}
		\vspace*{5pt}
		\centerline{\Large{}Inference}
		\vspace*{5pt}
		\begin{itemize}
			\item obtain probabilities $p(C_k|x)$
		\end{itemize}
		~\\%
	\end{minipage}
	\hfill\vrule\hfill
	\begin{minipage}[t]{.48\textwidth}
		\vspace*{5pt}
		\centerline{\Large{}Decision}
		\vspace*{5pt}
		\begin{itemize}
			\item obtain optimal class assignment $C^*$
		\end{itemize}
		~\\%
	\end{minipage}
\end{minipage}
%
%
%
\clearpage
\subsection{Solving decision problems}
There are three approaches commonly known to tackle decision problems: %
\\[.5cm]%
\begin{minipage}{1\textwidth}
	\begin{minipage}[t]{.32\textwidth}
		\centerline{\Large{}Generative models}
		\vspace*{5pt}
		\begin{itemize}
			\item Infer the class conditionals $p(x|C_k),~k=1,\ldots,K$
		\end{itemize}
	\end{minipage}
	\hfill\vrule\hfill
	\begin{minipage}[t]{.32\textwidth}
		\centerline{\Large{}Discriminative models}
		\vspace*{5pt}
		\begin{itemize}
			\item Infer posterior probabilities $p(C_k|x)$ directly
		\end{itemize}
	\end{minipage}
	\hfill\vrule\hfill
	\begin{minipage}[t]{.32\textwidth}
		\centerline{\Large{}Discriminative function}
		\vspace*{5pt}
		\begin{itemize}
			\item Find $f:\mathcal{X}\rightarrow\{1,\ldots,K\}$ by\\minimizing $\mathbb{E}\left[\Delta\right]$ directly\\where $\Delta$ is a given loss function\\
		\end{itemize}
	\end{minipage}
\end{minipage}
\hrule height 1pt%
~\\%
\begin{minipage}{1\textwidth}
	\begin{minipage}[t]{.32\textwidth}
		\vspace*{5pt}
%
		\begin{itemize}
			\item[\color{Green}\textbf{+}] We can generate samples from the learned distribution
			\item[\color{Green}\textbf{+}] We are able obtain the marginal probability $p(x) = \sum_k p(x|C_k)$
			\item[\color{Red}\textbf{--}] Need for lot of data for high dimensions of $x$ to determine class conditionals
			\item[\color{Red}\textbf{--}] We might not be interested in all quantities and this seems to be too much
		\end{itemize}
		\vspace*{5pt}
%
	\end{minipage}
	\hfill\vrule\hfill
	\begin{minipage}[t]{.32\textwidth}
		\vspace*{5pt}
		%
		\begin{itemize}
			\item[\color{Green}\textbf{+}] No need to model the class\\conditionals $p(x|C_k)$
			\item[]~\\
			\item[\color{Red}\textbf{--}] No access to model $p(x)$
			\item[]~\\
		\end{itemize}
		\vspace*{5pt}
		%
	\end{minipage}
	\hfill\vrule\hfill
	\begin{minipage}[t]{.32\textwidth}
		\vspace*{5pt}
		%
		\begin{itemize}
			\item[\color{Green}\textbf{+}] Models the quantitiy of interest directly
			\item[\color{Green}\textbf{+}] Not bound to probabilities and hence no need to normalize
			\item[\color{Red}\textbf{--}] Need for a loss function $\Delta$ while learning and changes to the loss function $\Delta$ requires re-learning
			\item[\color{Red}\textbf{--}] No probabilities mean no uncertainty and no reject option
		\end{itemize}
		\vspace*{5pt}
		%
	\end{minipage}
\end{minipage}
%
%
%